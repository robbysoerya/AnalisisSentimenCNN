{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDdQNbljpmxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Embedding, Conv1D\n",
        "from keras.layers import Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import timedelta\n",
        "\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLu2MjbspunH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "print(start_time)\n",
        "print('Training Word2Vec Model...')\n",
        "\n",
        "data = pd.read_csv('Hasil_Sentimen.csv',index_col=None)\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "x = vectorizer.fit_transform(data['tweets'].values)\n",
        "y = data['label'].values\n",
        "\n",
        "trainX, testX, trainy, testy = train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "# this returns a tensor\n",
        "print(\"Creating Model...\")\n",
        "verbose, epochs, batch_size = 1, 100, 250\n",
        "\n",
        "n_timesteps, n_features, n_outputs = trainX.shape[0], trainX.shape[1], trainy.shape[0]\n",
        "model = Sequential()\n",
        "model.add(Embedding(n_timesteps, batch_size, input_length=n_features))\n",
        "model.add(Conv1D(10,(5), activation='relu', padding='same'))\n",
        "model.add(Conv1D(10,(5), activation='relu', padding='same'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(n_outputs, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Traning Model...\")\n",
        "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data=(testX,testy))\n",
        "\t\n",
        "finish_time = time.time()\n",
        "\n",
        "model_json = model.to_json()\n",
        "\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "print('Finished. Elapsed time: {}'.format(timedelta(seconds=finish_time-start_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}